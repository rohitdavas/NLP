{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "heading_collapsed": true
   },
   "source": [
    "## Read Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets  ['/mnt/Study_Storage/Documents/Datasets/Text/Names', '/mnt/Study_Storage/Documents/Datasets/Text/Sentiment_Analysis', '/mnt/Study_Storage/Documents/Datasets/Text/Shakspeare']\n"
     ]
    }
   ],
   "source": [
    "Text_dataset_folder = \"/mnt/Study_Storage/Documents/Datasets/Text/\"\n",
    "filename = glob(Text_dataset_folder + \"*\")\n",
    "print(\"datasets \", filename)\n",
    "\n",
    "sentiment_folder = Text_dataset_folder + \"Sentiment_Analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_files = sentiment_folder + \"/Dataset_1/train/pos\"\n",
    "\n",
    "train_docs = glob(train_files + \"/*\")\n",
    "# print(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file location: \n",
      "/mnt/Study_Storage/Documents/Datasets/Text/Sentiment_Analysis/Dataset_1/train/pos/0_9.txt \n",
      " /mnt/Study_Storage/Documents/Datasets/Text/Sentiment_Analysis/Dataset_1/train/pos/10000_8.txt\n"
     ]
    }
   ],
   "source": [
    "#pic any two docs\n",
    "\n",
    "doc1 = train_docs[0]\n",
    "doc2 = train_docs[1]\n",
    "\n",
    "print(\"file location: \")\n",
    "print(doc1, \"\\n\", doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def read(filename):\n",
    "    with open(filename) as f:\n",
    "        file = f.read()\n",
    "    return file\n",
    "\n",
    "\n",
    "train1 = read(doc1)\n",
    "train2 = read(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of words in train1  140\n",
      "no of words in train2  428\n"
     ]
    }
   ],
   "source": [
    "# print(train1)\n",
    "# print(train2)\n",
    "\n",
    "# print(\"\\n\\n\")\n",
    "# print(\"read type\" , type(train1))\n",
    "\n",
    "print(\"no of words in train1 \", len(train1.split()))\n",
    "print(\"no of words in train2 \", len(train2.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def vocab(corpus):\n",
    "    vocub = []\n",
    "    for word in corpus.split():\n",
    "        if word not in vocub:\n",
    "            vocub.append(word)\n",
    "\n",
    "    return vocub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Method 1: one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of vocublary:  327\n",
      "(140, 327)\n",
      "(428, 327)\n"
     ]
    }
   ],
   "source": [
    "vocub = vocab(train1 + train2)\n",
    "length = len(vocub)\n",
    "print(\"len of vocublary: \", length)\n",
    "\n",
    "dictionary = {v: k for k, v in enumerate(vocub)}\n",
    "\n",
    "\n",
    "def one_hot_encoding(data, dictionary):\n",
    "    vocub_length = len(dictionary)\n",
    "    data_length = len(data.split())\n",
    "\n",
    "    one_hot = np.zeros([data_length, vocub_length])\n",
    "\n",
    "    for i, word in enumerate(data.split()):\n",
    "        if word in dictionary.keys():\n",
    "            index = dictionary[word]\n",
    "            one_hot[i][index] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "train1_hot = one_hot_encoding(train1, dictionary)\n",
    "print(train1_hot.shape)\n",
    "\n",
    "train2_hot = one_hot_encoding(train2, dictionary)\n",
    "print(train2_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Method 2: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0,
     14
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2041199826559248, 0.6020599913279624, 0.0, 0.0, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.6020599913279624, 0.0, 0.3010299956639812, 0.3010299956639812, 0.0, 0.3010299956639812, 0.0, 0.3010299956639812, 0.0, 0.3010299956639812, 0.3010299956639812, 0.0, 0.0, 0.6020599913279624, 0.3010299956639812, 0.3010299956639812, 0.0, 0.0, 0.0, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.6020599913279624, 0.0, 0.3010299956639812, 0.0, 1.2041199826559248, 0.3010299956639812, 0.3010299956639812, 0.0, 0.3010299956639812, 0.3010299956639812, 0.0, 0.3010299956639812, 0.3010299956639812, 0.0, 0.6020599913279624, 0.0, 0.3010299956639812, 0.0, 0.3010299956639812, 0.3010299956639812, 0.0, 0.3010299956639812, 0.3010299956639812, 0.0, 0.0, 0.0, 0.3010299956639812, 0.3010299956639812, 0.0, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.0, 0.3010299956639812, 0.0, 0.0, 0.3010299956639812, 0.3010299956639812, 0.0, 0.3010299956639812, 0.6020599913279624, 0.0, 0.0, 0.3010299956639812, 0.0, 0.3010299956639812, 0.0, 0.0, 0.3010299956639812, 0.3010299956639812, 0.0, 0.3010299956639812, 0.0, 0.3010299956639812, 0.0, 0.3010299956639812, 0.0, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.0, 0.3010299956639812, 0.3010299956639812, 0.0, 0.0, 0.0, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.6020599913279624, 0.3010299956639812, 0.6020599913279624, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.0, 0.3010299956639812, 0.0, 0.0, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.0, 1.2041199826559248, 0.6020599913279624, 0.0, 0.3010299956639812, 0.0, 0.3010299956639812, 0.3010299956639812, 0.0, 0.3010299956639812, 0.3010299956639812, 0.0, 0.0, 1.2041199826559248, 0.6020599913279624, 0.0, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.0, 0.3010299956639812, 0.0, 0.0, 0.3010299956639812]\n"
     ]
    }
   ],
   "source": [
    "def embedding_tfIdf(TF, IDF, docs):\n",
    "    docs_embed = []\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        tf = TF[i]\n",
    "        doc_embed = [\n",
    "            tf[word] * np.log10(len(docs) / IDF[word]) for word in doc.split()\n",
    "            if word in tf.keys()\n",
    "        ]\n",
    "        docs_embed.append(doc_embed)\n",
    "\n",
    "    return docs_embed\n",
    "\n",
    "\n",
    "def tfIdf(docs):\n",
    "    TF = []\n",
    "    IDF = {}\n",
    "\n",
    "    for doc in docs:\n",
    "        freq = {}\n",
    "\n",
    "        for word in doc.split():\n",
    "            if word not in freq:\n",
    "                freq[word] = 0\n",
    "            freq[word] += 1\n",
    "\n",
    "        for word in freq.keys():\n",
    "            if word not in IDF:\n",
    "                IDF[word] = 0\n",
    "            IDF[word] += 1\n",
    "\n",
    "        TF.append(freq)\n",
    "    docs_embed = embedding_tfIdf(TF, IDF, docs)\n",
    "\n",
    "    return TF, IDF, docs_embed\n",
    "\n",
    "\n",
    "docs = [train1, train2]\n",
    "TF, IDF, docs_embed = tfIdf(docs)\n",
    "print(docs_embed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Co-occurence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 5: Glove"
   ]
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Text preporcessing.ipynb",
    "public": true
   },
   "id": ""
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "346px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
